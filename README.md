ğŸ§  RAG Pipeline & Chatbot â€” Project Notes
ğŸ“˜ Overview

RAG (Retrieval-Augmented Generation) combines retrieval-based search with generative AI to produce informed, accurate answers.
The pipeline works by retrieving relevant information from a vector database and using an LLM to generate a coherent response.

    Retrieval â†’ Finds relevant context from stored documents.

    Augmentation â†’ Injects that context into the LLM query.

    Generation â†’ Produces an answer thatâ€™s grounded in the retrieved data.

ğŸ’¾ Understanding Vector Databases

    A vector database stores text as numerical embeddings (vectors).

    Each vector represents the semantic meaning of text, not just keywords.

    Similar meanings = vectors placed closer in multi-dimensional space.

Example:

    â€œWolfâ€, â€œDogâ€, and â€œCatâ€ â†’ clustered together (animals).

    â€œAppleâ€ and â€œBananaâ€ â†’ clustered together (fruits).

When a query like â€œkittenâ€ is asked:

    Itâ€™s converted to a vector using the same embedding model.

    The system retrieves nearby vectors (similar meanings).

    Those are used as context for the answer.

ğŸ§© Data Preparation Pipeline

Steps:

    Split documents into smaller chunks (since large files canâ€™t be embedded whole).

    Embed text using a model (e.g., OpenAI embeddings: text-embedding-3-small).

    Store embeddings in a vector database (Pinecone).

Chunks are organized by context (e.g., company data, marketing data, etc.), keeping semantic structure intact.
âš™ï¸ Workflow Setup (N8N)

1. Trigger Step (Google Drive Integration)

    Add trigger: â€œOn changes in a specific folder.â€

    Connect Google Drive via OAuth credentials:

        Create project in Google Cloud Console.

        Enable Drive API.

        Configure OAuth Consent Screen (External, test mode).

        Add redirect URI from N8N.

        Retrieve and input Client ID and Client Secret into N8N.

Result: Workflow starts when a new file is added to your chosen Google Drive folder (e.g., /FAQ/).

2. Download File Node

    Use Google Drive node â†’ Download File by File ID (dynamic reference).

    Output: Fileâ€™s binary data (the actual document).

3. Setup Pinecone Vector Store

    Go to pinecone.io.

    Create an index (e.g., sample).

        Use embeddings model: text-embedding-3-small.

        Choose serverless + AWS region.

    Create an API Key and connect it in N8N.

In N8N:

    Add Pinecone node â†’ â€œAdd documents to vector storeâ€.

    Define:

        Index: sample

        Namespace: FAQ

        Add Embeddings model + Document Loader + Text Splitter

        Split chunks (e.g., size = 1000 characters, overlap = 0)

Youâ€™ve now built the base RAG data ingestion pipeline!
ğŸ¤– Building the Chatbot (RAG Agent)

1. Create an AI Agent Node

    Add an AI agent to the workflow.

    Connect a Chat Trigger so messages entered trigger the agent.

2. Add LLM Model

    Use OpenRouter or OpenAI API:

        Sign up at openrouter.ai

        Connect via API Key.

        Choose from models (e.g., Claude 3.5 Sonnet or GPT-4).

3. Connect the Vector Store (Knowledge Base)

    Add Pinecone Vector Store as a tool under the AI agent:

        Name: knowledgebase

        Description: â€œAccesses policy and FAQ database.â€

        Index: sample

        Namespace: FAQ

        Embedding Model: text-embedding-3-small

ğŸ’¬ Testing the Chatbot

Ask:

    â€œWhat is our warranty policy?â€

    The chatbot queries Pinecone using vector search.

    Retrieves nearest chunks (context).

    LLM generates an answer incorporating that context.

Output Example:

    Based on our policy database: 1-year standard coverage, email for claims, exclusions for misuse or water damage.
